{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daniel-Brett-Stark/PortfolioProjects/blob/main/PDSLassoPortfolioProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgvz3CpnyzpI",
        "outputId": "706c9081-ceb6-42c8-8fe3-3c63e6239cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import df\n",
        "df=pd.read_csv('/content/gdrive/My Drive/Econ 484/484Project/MergedData/AllCountriesMergedDtypes.csv')\n",
        "print(df.head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV4l0D0hGqwk",
        "outputId": "2c47c095-39ca-4d11-da53-0ebe694fe661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of       Unnamed: 0    value1     value2      value3  value4    value5  value6  \\\n",
            "0              0  153.9510  15.449556   55.813258  12.400  0.037055    5.30   \n",
            "1              1  150.1920  17.990082  170.825030  79.478  0.037488   32.64   \n",
            "2              2  146.4330  38.627892   43.015907  35.840  0.048671   18.28   \n",
            "3              3  143.7648  37.418855   31.488022  14.300  0.057008   10.30   \n",
            "4              4  141.0966  29.721067   37.386922  42.080  0.045700   24.34   \n",
            "...          ...       ...        ...         ...     ...       ...     ...   \n",
            "1492        1492  104.3014   7.144479   24.841702  61.580  0.925816   33.18   \n",
            "1493        1493   99.7598   8.745304   24.841702  80.000  0.894256   67.00   \n",
            "1494        1494   86.1350   8.340969   27.234910  68.240  0.718570   33.08   \n",
            "1495        1495   83.2486  10.520955   26.046247  77.660  0.849793   40.18   \n",
            "1496        1496   80.3622  10.143657   13.209815  86.000  0.099858   33.08   \n",
            "\n",
            "         value7       value8       value9  ...    value51    value52  value53  \\\n",
            "0     79.053020  2368.342423  1530.927355  ...  14.602391   1.496845     74.3   \n",
            "1     19.382150  2273.087108  1391.442623  ...  16.565917  10.320229     22.8   \n",
            "2     41.650937  1899.746151  1097.036525  ...  10.738924   7.761681     28.4   \n",
            "3     21.000120  1899.746151  1097.036525  ...   8.359550  10.125830     13.4   \n",
            "4     26.584104  1911.301579  1089.961066  ...   4.789492  10.298449      9.5   \n",
            "...         ...          ...          ...  ...        ...        ...      ...   \n",
            "1492  45.149323   628.746242   845.359435  ...  15.238402   9.774420     91.0   \n",
            "1493  49.260685   609.124652   716.101420  ...  16.299974   9.774420     91.0   \n",
            "1494  63.135671  6427.067984  8078.140991  ...  15.874375  27.214585     61.0   \n",
            "1495  58.991774  6298.362283  8006.832944  ...  13.603126  27.214585     32.0   \n",
            "1496  63.865248  6427.067984  8078.140991  ...  17.515783  20.186751     27.0   \n",
            "\n",
            "        value54    value55   value56    year  countrycode  nkill  success  \n",
            "0     20.702743  30.370000  3.392644  2000.0          AFG   38.0     14.0  \n",
            "1     11.060501  30.370000  4.314130  2001.0          AFG  174.0     14.0  \n",
            "2      8.435163  40.380000  4.910849  2002.0          AFG   74.0     28.0  \n",
            "3      4.245002  40.380000  5.080775  2003.0          AFG  163.0     93.0  \n",
            "4      1.213615  40.380000  4.977038  2004.0          AFG  275.0     79.0  \n",
            "...         ...        ...       ...     ...          ...    ...      ...  \n",
            "1492  14.051574   6.376000  1.227943  2013.0          ZWE    1.0      3.0  \n",
            "1493  12.662905   4.360000  1.294326  2014.0          ZWE    0.0      1.0  \n",
            "1494  15.405025  11.888000  1.276544  2017.0          ZWE    0.0      3.0  \n",
            "1495  11.687913  15.876000  1.323497  2018.0          ZWE    2.0      1.0  \n",
            "1496  30.053505  16.040001  1.424249  2019.0          ZWE    0.0      2.0  \n",
            "\n",
            "[1497 rows x 61 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.loc[:,'success']\n",
        "d = df.loc[:,'value55']\n",
        "X = df.drop(columns=['success', 'value55','countrycode','Unnamed: 0'],axis=1)\n",
        "\n",
        "#step 1\n",
        "lassoy = LassoCV(normalize=True).fit(X, y)\n",
        "\n",
        "#step 2\n",
        "lassod = LassoCV(normalize=True).fit(X, d)\n",
        "print(lassod.coef_)\n",
        "\n",
        "lassod = LassoCV(normalize=True).fit(X, d)\n",
        "\n",
        "#step 3\n",
        "Xunion=X.iloc[:,(lassod.coef_!=0) + (lassoy.coef_!=0)]\n",
        "Xunion.head()\n",
        "\n",
        "rhs=pd.concat([d,Xunion],axis=1)\n",
        "fullreg=linear_model.LinearRegression().fit(rhs,y)\n",
        "print(\"PDS regression effect of selective college: {:.3f}\".format(fullreg.coef_[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T253OXnx-Lqa",
        "outputId": "e616a52a-f2db-41eb-f4c2-b336817f6e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          0.          0.         -0.          0.         -0.\n",
            " -0.         -0.         -0.          0.          0.          0.\n",
            " -0.         -0.         -0.          0.         -0.         -0.\n",
            " -0.          0.         -0.         -0.         -0.          0.\n",
            "  0.          0.          0.         -0.          0.          0.\n",
            " -0.          0.          0.         -0.          0.         -0.\n",
            "  0.          0.          0.          0.          0.02939885  0.\n",
            "  0.         -0.         -0.         -0.         -0.         -0.01090299\n",
            " -0.          0.         -0.          0.          0.         -0.\n",
            "  0.         -0.          0.        ]\n",
            "PDS regression effect of selective college: 0.150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PDS regression effect of unemployment on terrorist attacks: {:.3f}\".format(fullreg.coef_[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt2MNRXCkHD7",
        "outputId": "1f9623ed-a72c-4f1e-d170-07fa1294c64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDS regression effect of unemployment on terrorist attacks: 0.150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create our sample splitting \"object\"\n",
        "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
        "\n",
        "# apply the splits to our Xs\n",
        "kf.get_n_splits(X)\n",
        "\n",
        "# initialize array to hold each fold's regression coefficient\n",
        "coeffs=np.zeros(5)\n",
        "\n",
        "# Now loop through each fold\n",
        "ii=0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "  d_train, d_test = d.iloc[train_index], d.iloc[test_index]\n",
        "\n",
        "\n",
        "  # Do DDML thing\n",
        "  # Ridge y on training folds:\n",
        "  lassoy.fit(X_train, y_train)\n",
        "\n",
        "  # but get residuals in test set\n",
        "  yresid=y_test-lassoy.predict(X_test)\n",
        "  \n",
        "  #Ridge d on training folds\n",
        "  lassod.fit(X_train,d_train)\n",
        "\n",
        "  #but get residuals in test set\n",
        "  dresid=d_test-lassod.predict(X_test)\n",
        "  dresid=np.array(dresid).reshape(-1,1)\n",
        "  \n",
        "  # regress resids on resids\n",
        "  ddmlreg=linear_model.LinearRegression().fit(dresid,yresid)\n",
        "\n",
        "  # save coefficient in a vector\n",
        "  coeffs[ii]=ddmlreg.coef_[0]\n",
        "  ii+=1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtPJQZpyG9in",
        "outputId": "9f2fa215-785b-4810-ded8-b04caf3b8fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70.17926285063731, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 261.9005937290058, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 364.2706385776037, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 407.69269054614415, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 424.5714019630468, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 430.47120372702193, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 431.9773816286033, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 431.76966214945423, tolerance: 11.664372202459289\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 62.38625934715674, tolerance: 12.654816241177112\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:644: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 177.66362970414048, tolerance: 12.654816241177112\n",
            "  positive,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_base.py:145: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
            "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
            "\n",
            "from sklearn.pipeline import make_pipeline\n",
            "\n",
            "model = make_pipeline(StandardScaler(with_mean=False), Lasso())\n",
            "\n",
            "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
            "\n",
            "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
            "model.fit(X, y, **kwargs)\n",
            "\n",
            "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Take average\n",
        "print(\"Double-Debiased Machine Learning effect of unemployment on terrorism: {:.3f}\".format(np.mean(coeffs)))\n",
        "coeffs"
      ],
      "metadata": {
        "id": "VTsAhXH5Vuk9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5b222d-27b6-4093-9bf5-aa4cd45c1ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Double-Debiased Machine Learning effect of recessions on terrorism: 0.268\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.79826372, -0.12834875,  0.69493336,  0.19066353,  1.3796542 ])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}